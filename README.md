
Contemporary data acquisition methods consistently yield voluminous event sequence data across numerous domains,such as social media, healthcare, and financial markets. These data often exhibit complex short- and long-term temporal dependencies. However, most existing recurrent neural network-based point process models fail to capture these dependencies, resulting in unreliable prediction performance.
To address this issue, we propose the TransTPP model, which leverages the self-attention mechanism to capture long-term dependencies while maintaining computational efficiency. Numerical experiments on various datasets demonstrate that the TransTPP model outperforms existing models in terms of both likelihood and event prediction accuracy by a significant margin. Furthermore, the TransTPP model is quite general and can be extended to incorporate additional structural knowledge. We provide a concrete example where the TransTPP model achieves improved prediction performance for learning multiple point processes when incorporating their relational information.

